{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfp BFGS conv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuhiG/MNIST-in-Tensorflow/blob/master/tfp_BFGS_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Z6s19M4BTTul",
        "colab_type": "code",
        "outputId": "36b2cdde-8db3-4ea8-9e90-251045972fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
        "#one_hot encoding is to make the data more machine readable\n",
        "\n",
        "#hyper parameters\n",
        "\n",
        "learning_rate=0.01\n",
        "training_iters=10000\n",
        "batch_size=128#samples\n",
        "display_step=10 #every 10 iteration, display\n",
        "\n",
        "#network paras\n",
        "n_input=784#image shape is gpoing to be 28*28\n",
        "n_classes=10 #10 digits 0~9\n",
        "dropout=0.75 #prevents overfitting by randomly turning off some neurons during training,so the data forced to find new paths between layers to generalize the model\n",
        "#ex: old people explanation\n",
        "\n",
        "x=tf.placeholder(tf.float32,[None,n_input])#images\n",
        "y=tf.placeholder(tf.float32,[None,n_classes])#labels\n",
        "keep_prob=tf.placeholder(tf.float32)\n",
        "\n",
        "#creating the C layers\n",
        "\n",
        "def conv2d(x,w,b,strides=1):\n",
        "\tx=tf.nn.conv2d(x,w,strides=[1,strides,strides,1], padding='SAME')\n",
        "\tx =tf.nn.bias_add(x, b)\n",
        "\treturn(tf.nn.relu(x))\n",
        "#convolution is tranforming it o some way(putting filters)\n",
        "#must have some thing about original image\n",
        "#bias makes the model more accurate\n",
        "#strides=list of ints / tensors=data\n",
        "\n",
        "def maxpool2d(x,k=2):#pooling=small rectangular blocks from the conv layer and sub samples them little pools from the image.\n",
        "\treturn tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding=\"SAME\")\n",
        "\n",
        "\n",
        "#creating the model\n",
        "\n",
        "def conv_net(x,weights,biases,dropout):\n",
        "\tx=tf.reshape(x,shape=[-1,28,28,1])\n",
        "\n",
        "\t#conv layer\n",
        "\tconv1=conv2d(x,weights[\"wc1\"],biases[\"bc1\"])\n",
        "\t#max pooling layer\n",
        "\tconv1=maxpool2d(conv1,k=2)\n",
        "\n",
        "\tconv2=conv2d(conv1,weights[\"wc2\"],biases[\"bc2\"])\n",
        "\tconv2=maxpool2d(conv2,k=2)\n",
        "\n",
        "\t#fully connected layer\n",
        "\tfc1=tf.reshape(conv2,[-1,weights[\"wd1\"].get_shape().as_list()[0]])\n",
        "\t#matrix mul\n",
        "\tfc1=tf.add(tf.matmul(fc1,weights[\"wd1\"]),biases[\"bd1\"])\n",
        "\tfc1=tf.nn.relu(fc1)\n",
        "\n",
        "\t#applying dropout\n",
        "\tfc1=tf.nn.dropout(fc1,dropout)\n",
        "\n",
        "\t#output is going to predict our class\n",
        "\tout=tf.add(tf.matmul(fc1,weights[\"out\"]),biases[\"out\"])\n",
        "\treturn out\n",
        "\n",
        "#creating weights\n",
        "weights={\n",
        "\"wc1\":tf.Variable(tf.random_normal([5,5,1,32])),#5*5,1=inputs,32=bits \n",
        "\"wc2\":tf.Variable(tf.random_normal([5,5,32,64])),#5*5,32=inputs,64=bits \n",
        "\"wd1\":tf.Variable(tf.random_normal([7*7*64,1024])),#7*7*64=inputs,1024=bits /outputs\n",
        "\"out\":tf.Variable(tf.random_normal([1024,n_classes]))#1024=inputs,number of classes=10\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "#construct model\n",
        "pred=conv_net(x,weights,biases,keep_prob)#keep_prob=dropout\n",
        "\n",
        "#deine optimizer and loss\n",
        "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))#measuring the probability error in a classification task /mutually exclusive one\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "optimizer=tfp.optimizer.bfgs_minimize(cost,initial_position=accuracy,tolerance=1e-08)\n",
        "# Initializing the variables\n",
        "init =tf.global_variables_initializer()#everytime initializing a tf graph, must initialize variables\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 1\n",
        "    # Keep training until reach max iterations\n",
        "    while step < training_iters:\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
        "                                       keep_prob: dropout})\n",
        "        if step % display_step == 0:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y,\n",
        "                                                              keep_prob: 1.})\n",
        "            print(\"Iterations \" + str(step) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.5f}\".format(acc))\n",
        "        step += 1\n",
        "    print(\"Optimization Completed!\")\n",
        "\n",
        "    # Calculate accuracy for 256 mnist test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
        "                                      y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a501f647c8fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mcorrect_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfgs_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;31m# Initializing the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#everytime initializing a tf graph, must initialize variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/optimizer/bfgs.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(value_and_gradients_function, initial_position, tolerance, x_tolerance, f_relative_tolerance, initial_inverse_hessian_estimate, max_iterations, parallel_iterations, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m       \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_gradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0minitial_convergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initial_convergence_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "163-Ymofbavn",
        "colab_type": "code",
        "outputId": "3f7c56a0-df95-4b4e-d17d-16a9beb54afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q skflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for skflow (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}